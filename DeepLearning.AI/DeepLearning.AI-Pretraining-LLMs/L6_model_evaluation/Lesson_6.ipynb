{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ae2a8fff-4124-4c6d-9258-c05657ec01c8",
   "metadata": {},
   "source": [
    "# Lesson 6. Model evaluation\n",
    "\n",
    "The model comparison tool that Sung described in the video can be found at this link: https://console.upstage.ai/ (note that you need to create a free account to try it out.)\n",
    "\n",
    "A useful tool for evaluating LLMs is the **LM Evaluation Harness** built by EleutherAI. Information about the harness can be found at this [github repo](https://github.com/EleutherAI/lm-evaluation-harness):\n",
    "\n",
    "You can run the commented code below to install the evaluation harness in your own environment:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b31fbf66-c7d5-4323-8f8a-acda7a12f66b",
   "metadata": {
    "height": 30
   },
   "outputs": [],
   "source": [
    "#!pip install -U git+https://github.com/EleutherAI/lm-evaluation-harness"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac0b1cfc",
   "metadata": {},
   "source": [
    "You will evaluate TinySolar-248m-4k on 5 questions from the **TruthfulQA MC2 task**. This is a multiple-choice question answering task that tests the model's ability to identify true statements. You can read more about the TruthfulQA benchmark in [this paper](https://arxiv.org/abs/2109.07958), and you can checkout the code for implementing the tasks at this [github repo](https://github.com/sylinrl/TruthfulQA).\n",
    "\n",
    "The code below runs only the TruthfulQA MC2 task using the LM Evaluation Harness:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "55c2c258-98de-43c7-9b96-cce7a6f20024",
   "metadata": {
    "height": 98
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-07-26:07:59:09,138 INFO     [__main__.py:272] Verbosity set to INFO\n",
      "2024-07-26:07:59:09,301 INFO     [__init__.py:403] `group` and `group_alias` keys in tasks' configs will no longer be used in the next release of lm-eval. `tag` will be used to allow to call a collection of tasks just like `group`. `group` will be removed in order to not cause confusion with the new ConfigurableGroup which will be the offical way to create groups with addition of group-wide configuations.\n",
      "2024-07-26:07:59:14,254 WARNING  [__main__.py:312]  --limit SHOULD ONLY BE USED FOR TESTING.REAL METRICS SHOULD NOT BE COMPUTED USING LIMIT.\n",
      "2024-07-26:07:59:14,254 INFO     [__main__.py:369] Selected Tasks: ['truthfulqa_mc2']\n",
      "2024-07-26:07:59:14,255 INFO     [evaluator.py:158] Setting random seed to 0 | Setting numpy seed to 1234 | Setting torch manual seed to 1234\n",
      "2024-07-26:07:59:14,255 INFO     [evaluator.py:195] Initializing hf model, with arguments: {'pretrained': './models/upstage/TinySolar-248m-4k'}\n",
      "2024-07-26:07:59:14,258 WARNING  [logging.py:61] Detected kernel version 4.18.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.\n",
      "2024-07-26:07:59:14,258 INFO     [huggingface.py:170] Using device 'cpu'\n",
      "Downloading readme: 100%|██████████████████| 9.59k/9.59k [00:00<00:00, 44.1MB/s]\n",
      "Downloading data: 100%|██████████████████████| 271k/271k [00:00<00:00, 1.54MB/s]\n",
      "Generating validation split: 100%|█| 817/817 [00:00<00:00, 114183.01 examples/s]\n",
      "2024-07-26:07:59:18,594 INFO     [evaluator.py:274] Setting fewshot random generator seed to 1234\n",
      "2024-07-26:07:59:18,596 INFO     [task.py:423] Building contexts for truthfulqa_mc2 on rank 0...\n",
      "100%|████████████████████████████████████████████| 5/5 [00:00<00:00, 777.10it/s]\n",
      "2024-07-26:07:59:18,603 INFO     [evaluator.py:457] Running loglikelihood requests\n",
      "Running loglikelihood requests: 100%|███████████| 33/33 [02:00<00:00,  3.65s/it]\n",
      "2024-07-26:08:01:19,221 WARNING  [huggingface.py:1314] Failed to get model SHA for ./models/upstage/TinySolar-248m-4k at revision main. Error: Repo id must be in the form 'repo_name' or 'namespace/repo_name': './models/upstage/TinySolar-248m-4k'. Use `repo_type` argument if needed.\n",
      "2024-07-26:08:01:20,192 INFO     [evaluation_tracker.py:269] Output path not provided, skipping saving results aggregated\n",
      "hf (pretrained=./models/upstage/TinySolar-248m-4k), gen_kwargs: (None), limit: 5.0, num_fewshot: None, batch_size: 1\n",
      "|    Tasks     |Version|Filter|n-shot|Metric|   |Value |   |Stderr|\n",
      "|--------------|------:|------|-----:|------|---|-----:|---|-----:|\n",
      "|truthfulqa_mc2|      2|none  |     0|acc   |↑  |0.4008|±  |0.2446|\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!lm_eval --model hf \\\n",
    "    --model_args pretrained=./models/upstage/TinySolar-248m-4k \\\n",
    "    --tasks truthfulqa_mc2 \\\n",
    "    --device cpu \\\n",
    "    --limit 5"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbeef916",
   "metadata": {},
   "source": [
    "### Evaluation for the Hugging Face Leaderboard\n",
    "You can use the code below to test your own model against the evaluations required for the [Hugging Face leaderboard](https://huggingface.co/open-llm-leaderboard). \n",
    "\n",
    "If you decide to run this evaluation on your own model, don't change the few-shot numbers below - they are set by the rules of the leaderboard."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cc097be-7dbb-4a90-a954-f39d30e8e52c",
   "metadata": {
    "height": 404
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-07-26:08:02:39,736 INFO     [__main__.py:272] Verbosity set to INFO\n",
      "2024-07-26:08:02:39,897 INFO     [__init__.py:403] `group` and `group_alias` keys in tasks' configs will no longer be used in the next release of lm-eval. `tag` will be used to allow to call a collection of tasks just like `group`. `group` will be removed in order to not cause confusion with the new ConfigurableGroup which will be the offical way to create groups with addition of group-wide configuations.\n",
      "2024-07-26:08:02:44,743 INFO     [__main__.py:369] Selected Tasks: ['arc_challenge']\n",
      "2024-07-26:08:02:44,743 INFO     [evaluator.py:158] Setting random seed to 0 | Setting numpy seed to 1234 | Setting torch manual seed to 1234\n",
      "2024-07-26:08:02:44,744 INFO     [evaluator.py:195] Initializing hf model, with arguments: {'pretrained': './models/upstage/TinySolar-248m-4k'}\n",
      "2024-07-26:08:02:44,746 WARNING  [logging.py:61] Detected kernel version 4.18.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.\n",
      "2024-07-26:08:02:44,746 INFO     [huggingface.py:170] Using device 'cpu'\n",
      "Downloading readme: 100%|██████████| 9.00k/9.00k [00:00<00:00, 28.7MB/s]\n",
      "Downloading data: 100%|██████████| 190k/190k [00:00<00:00, 1.92MB/s]\n",
      "Downloading data: 100%|██████████| 204k/204k [00:00<00:00, 2.28MB/s]\n",
      "Downloading data: 100%|██████████| 55.7k/55.7k [00:00<00:00, 664kB/s]\n",
      "Generating train split: 100%|██████████| 1119/1119 [00:00<00:00, 159863.28 examples/s]\n",
      "Generating test split: 100%|██████████| 1172/1172 [00:00<00:00, 429058.59 examples/s]\n",
      "Generating validation split: 100%|██████████| 299/299 [00:00<00:00, 182016.97 examples/s]\n",
      "2024-07-26:08:02:49,007 WARNING  [evaluator.py:262] Overwriting default num_fewshot of arc_challenge from None to 25\n",
      "2024-07-26:08:02:49,007 INFO     [evaluator.py:274] Setting fewshot random generator seed to 1234\n",
      "2024-07-26:08:02:49,007 INFO     [task.py:423] Building contexts for arc_challenge on rank 0...\n",
      "100%|██████████| 1172/1172 [00:41<00:00, 28.38it/s]\n",
      "2024-07-26:08:03:30,380 INFO     [evaluator.py:457] Running loglikelihood requests\n",
      "Running loglikelihood requests:   0%|          | 3/4687 [00:59<25:26:28, 19.55s/it]Traceback (most recent call last):\n",
      "  File \"/usr/local/bin/lm_eval\", line 8, in <module>\n",
      "    sys.exit(cli_evaluate())\n",
      "             ^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/site-packages/lm_eval/__main__.py\", line 375, in cli_evaluate\n",
      "    results = evaluator.simple_evaluate(\n",
      "              ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/site-packages/lm_eval/utils.py\", line 395, in _wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/site-packages/lm_eval/evaluator.py\", line 296, in simple_evaluate\n",
      "    results = evaluate(\n",
      "              ^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/site-packages/lm_eval/utils.py\", line 395, in _wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/site-packages/lm_eval/evaluator.py\", line 468, in evaluate\n",
      "    resps = getattr(lm, reqtype)(cloned_reqs)\n",
      "            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/site-packages/lm_eval/api/model.py\", line 371, in loglikelihood\n",
      "    return self._loglikelihood_tokens(new_reqs, disable_tqdm=disable_tqdm)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/site-packages/lm_eval/models/huggingface.py\", line 1086, in _loglikelihood_tokens\n",
      "    self._model_call(batched_inps, **call_kwargs), dim=-1\n",
      "    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/site-packages/lm_eval/models/huggingface.py\", line 801, in _model_call\n",
      "    return self.model(inps).logits\n",
      "           ^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 1518, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 1527, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py\", line 1183, in forward\n",
      "    outputs = self.model(\n",
      "              ^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 1518, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 1527, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py\", line 1070, in forward\n",
      "    layer_outputs = decoder_layer(\n",
      "                    ^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 1518, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 1527, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py\", line 812, in forward\n",
      "    hidden_states = self.mlp(hidden_states)\n",
      "                    ^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 1518, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 1527, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py\", line 268, in forward\n",
      "    down_proj = self.down_proj(self.act_fn(self.gate_proj(x)) * self.up_proj(x))\n",
      "                                           ^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 1518, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 1527, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/site-packages/torch/nn/modules/linear.py\", line 114, in forward\n",
      "    return F.linear(input, self.weight, self.bias)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "KeyboardInterrupt\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/bin/lm_eval\", line 3, in <module>\n",
      "    import re\n",
      "  File \"/usr/local/lib/python3.11/re/__init__.py\", line 124, in <module>\n",
      "    import enum\n",
      "  File \"/usr/local/lib/python3.11/enum.py\", line 5, in <module>\n",
      "    from functools import reduce\n",
      "  File \"/usr/local/lib/python3.11/functools.py\", line 18, in <module>\n",
      "    from collections import namedtuple\n",
      "KeyboardInterrupt\n",
      "Traceback (most recent call last):\n",
      "  File \"<frozen importlib._bootstrap_external>\", line 1440, in _path_importer_cache\n",
      "KeyError: '/usr/local/lib/python3.11/site-packages/torch/fx/experimental'\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/bin/lm_eval\", line 5, in <module>\n",
      "    from lm_eval.__main__ import cli_evaluate\n",
      "  File \"/usr/local/lib/python3.11/site-packages/lm_eval/__init__.py\", line 1, in <module>\n",
      "    from .evaluator import evaluate, simple_evaluate\n",
      "  File \"/usr/local/lib/python3.11/site-packages/lm_eval/evaluator.py\", line 10, in <module>\n",
      "    import torch\n",
      "  File \"/usr/local/lib/python3.11/site-packages/torch/__init__.py\", line 1726, in <module>\n",
      "    from torch import export as export\n",
      "  File \"/usr/local/lib/python3.11/site-packages/torch/export/__init__.py\", line 16, in <module>\n",
      "    from torch.fx.experimental.symbolic_shapes import StrictMinMaxConstraint\n",
      "  File \"<frozen importlib._bootstrap>\", line 1176, in _find_and_load\n",
      "  File \"<frozen importlib._bootstrap>\", line 1138, in _find_and_load_unlocked\n",
      "  File \"<frozen importlib._bootstrap>\", line 1078, in _find_spec\n",
      "  File \"<frozen importlib._bootstrap_external>\", line 1507, in find_spec\n",
      "  File \"<frozen importlib._bootstrap_external>\", line 1476, in _get_spec\n",
      "  File \"<frozen importlib._bootstrap_external>\", line 1442, in _path_importer_cache\n",
      "  File \"<frozen importlib._bootstrap_external>\", line 1420, in _path_hooks\n",
      "KeyboardInterrupt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-07-26:08:04:56,989 INFO     [__main__.py:272] Verbosity set to INFO\n",
      "2024-07-26:08:04:57,159 INFO     [__init__.py:403] `group` and `group_alias` keys in tasks' configs will no longer be used in the next release of lm-eval. `tag` will be used to allow to call a collection of tasks just like `group`. `group` will be removed in order to not cause confusion with the new ConfigurableGroup which will be the offical way to create groups with addition of group-wide configuations.\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/bin/lm_eval\", line 8, in <module>\n",
      "    sys.exit(cli_evaluate())\n",
      "             ^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/site-packages/lm_eval/__main__.py\", line 304, in cli_evaluate\n",
      "    task_manager = TaskManager(args.verbosity, include_path=args.include_path)\n",
      "                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/site-packages/lm_eval/tasks/__init__.py\", line 34, in __init__\n",
      "    self._task_index = self.initialize_tasks(\n",
      "                       ^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/site-packages/lm_eval/tasks/__init__.py\", line 67, in initialize_tasks\n",
      "    tasks = self._get_task_and_group(task_dir)\n",
      "            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/site-packages/lm_eval/tasks/__init__.py\", line 363, in _get_task_and_group\n",
      "    config = utils.load_yaml_config(yaml_path, mode=\"simple\")\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/site-packages/lm_eval/utils.py\", line 431, in load_yaml_config\n",
      "    yaml_config = yaml.full_load(file)\n",
      "                  ^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/site-packages/yaml/__init__.py\", line 105, in full_load\n",
      "    return load(stream, FullLoader)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/site-packages/yaml/__init__.py\", line 81, in load\n",
      "    return loader.get_single_data()\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/site-packages/yaml/constructor.py\", line 49, in get_single_data\n",
      "    node = self.get_single_node()\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/site-packages/yaml/composer.py\", line 36, in get_single_node\n",
      "    document = self.compose_document()\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/site-packages/yaml/composer.py\", line 55, in compose_document\n",
      "    node = self.compose_node(None, None)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/site-packages/yaml/composer.py\", line 84, in compose_node\n",
      "    node = self.compose_mapping_node(anchor)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/site-packages/yaml/composer.py\", line 133, in compose_mapping_node\n",
      "    item_value = self.compose_node(node, item_key)\n",
      "                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/site-packages/yaml/composer.py\", line 84, in compose_node\n",
      "    node = self.compose_mapping_node(anchor)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/site-packages/yaml/composer.py\", line 133, in compose_mapping_node\n",
      "    item_value = self.compose_node(node, item_key)\n",
      "                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/site-packages/yaml/composer.py\", line 82, in compose_node\n",
      "    node = self.compose_sequence_node(anchor)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/site-packages/yaml/composer.py\", line 111, in compose_sequence_node\n",
      "    node.value.append(self.compose_node(node, index))\n",
      "                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/site-packages/yaml/composer.py\", line 84, in compose_node\n",
      "    node = self.compose_mapping_node(anchor)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/site-packages/yaml/composer.py\", line 133, in compose_mapping_node\n",
      "    item_value = self.compose_node(node, item_key)\n",
      "                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/site-packages/yaml/composer.py\", line 64, in compose_node\n",
      "    if self.check_event(AliasEvent):\n",
      "       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/site-packages/yaml/parser.py\", line 98, in check_event\n",
      "    self.current_event = self.state()\n",
      "                         ^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/site-packages/yaml/parser.py\", line 572, in parse_flow_mapping_value\n",
      "    if not self.check_token(FlowEntryToken, FlowMappingEndToken):\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/site-packages/yaml/scanner.py\", line 116, in check_token\n",
      "    self.fetch_more_tokens()\n",
      "  File \"/usr/local/lib/python3.11/site-packages/yaml/scanner.py\", line 251, in fetch_more_tokens\n",
      "    return self.fetch_double()\n",
      "           ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/site-packages/yaml/scanner.py\", line 655, in fetch_double\n",
      "    self.fetch_flow_scalar(style='\"')\n",
      "  File \"/usr/local/lib/python3.11/site-packages/yaml/scanner.py\", line 666, in fetch_flow_scalar\n",
      "    self.tokens.append(self.scan_flow_scalar(style))\n",
      "                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/site-packages/yaml/scanner.py\", line 1152, in scan_flow_scalar\n",
      "    chunks.extend(self.scan_flow_scalar_non_spaces(double, start_mark))\n",
      "                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/site-packages/yaml/scanner.py\", line 1194, in scan_flow_scalar_non_spaces\n",
      "    self.forward(length)\n",
      "  File \"/usr/local/lib/python3.11/site-packages/yaml/reader.py\", line 102, in forward\n",
      "    while length:\n",
      "KeyboardInterrupt\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/bin/lm_eval\", line 5, in <module>\n",
      "    from lm_eval.__main__ import cli_evaluate\n",
      "  File \"/usr/local/lib/python3.11/site-packages/lm_eval/__init__.py\", line 1, in <module>\n",
      "    from .evaluator import evaluate, simple_evaluate\n",
      "  File \"/usr/local/lib/python3.11/site-packages/lm_eval/evaluator.py\", line 9, in <module>\n",
      "    import numpy as np\n",
      "  File \"/usr/local/lib/python3.11/site-packages/numpy/__init__.py\", line 113, in <module>\n",
      "    from . import version\n",
      "  File \"<frozen importlib._bootstrap>\", line 405, in parent\n",
      "KeyboardInterrupt\n",
      "2024-07-26:08:05:02,963 INFO     [__main__.py:272] Verbosity set to INFO\n",
      "2024-07-26:08:05:03,140 INFO     [__init__.py:403] `group` and `group_alias` keys in tasks' configs will no longer be used in the next release of lm-eval. `tag` will be used to allow to call a collection of tasks just like `group`. `group` will be removed in order to not cause confusion with the new ConfigurableGroup which will be the offical way to create groups with addition of group-wide configuations.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "def h6_open_llm_leaderboard(model_name):\n",
    "  task_and_shot = [\n",
    "      ('arc_challenge', 25),\n",
    "      ('hellaswag', 10),\n",
    "      ('mmlu', 5),\n",
    "      ('truthfulqa_mc2', 0),\n",
    "      ('winogrande', 5),\n",
    "      ('gsm8k', 5)\n",
    "  ]\n",
    "\n",
    "  for task, fewshot in task_and_shot:\n",
    "    eval_cmd = f\"\"\"\n",
    "    lm_eval --model hf \\\n",
    "        --model_args pretrained={model_name} \\\n",
    "        --tasks {task} \\\n",
    "        --device cpu \\\n",
    "        --num_fewshot {fewshot}\n",
    "    \"\"\"\n",
    "    os.system(eval_cmd)\n",
    "\n",
    "h6_open_llm_leaderboard(model_name=\"./models/upstage/TinySolar-248m-4k\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
